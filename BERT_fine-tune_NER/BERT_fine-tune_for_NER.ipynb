{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path=\"/data/home/arunkumar12/.cache/kagglehub/datasets/naseralqaydeh/named-entity-recognition-ner-corpus/versions/3\"\n",
    "data=pd.read_csv(f'{path}/ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag to ID mapping: {'B-tim': 0, 'I-tim': 1, 'I-geo': 2, 'B-gpe': 3, 'B-org': 4, 'I-gpe': 5, 'I-eve': 6, 'I-art': 7, 'I-per': 8, 'B-per': 9, 'I-org': 10, 'B-art': 11, 'B-geo': 12, 'O': 13, 'I-nat': 14, 'B-eve': 15, 'B-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "data['POS'] = data['POS'].apply(ast.literal_eval)\n",
    "data['Tag'] = data['Tag'].apply(ast.literal_eval)\n",
    "\n",
    "unique_tags = set(tag for doc in data['Tag'] for tag in doc)\n",
    "\n",
    "tag_to_id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
    "\n",
    "print(f\"Tag to ID mapping: {tag_to_id}\")\n",
    "data['Tag'] = data['Tag'].apply(lambda x: [tag_to_id[tag] for tag in x])\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    if not (len(row['Sentence'].split()) == len(row['POS']) == len(row['Tag'])):\n",
    "        data.drop(i,inplace=True)\n",
    "for i, row in data.iterrows():\n",
    "    assert len(row['Sentence'].split()) == len(row['POS']) == len(row['Tag']), f\"Row {i} has mismatch in lengths.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 47955\n",
      "Number of label sets: 47955\n",
      "Tokenized inputs keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "data['Sentence'] = data['Sentence'].apply(lambda x: x.split())\n",
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    print(f\"Number of sentences: {len(sentences)}\")\n",
    "    print(f\"Number of label sets: {len(labels)}\")\n",
    "    tokenized_inputs = tokenizer(sentences, truncation=True, is_split_into_words=True, return_offsets_mapping=True, padding=True)\n",
    "    \n",
    "    print(f\"Tokenized inputs keys: {tokenized_inputs.keys()}\")\n",
    "\n",
    "    offset_mappings = tokenized_inputs.pop(\"offset_mapping\")\n",
    "    aligned_labels = []\n",
    "    for i, offset_mapping in enumerate(offset_mappings):\n",
    "        label_ids = []\n",
    "        label = labels[i]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  \n",
    "            else:\n",
    "                label_ids.append(-100)  \n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "sentences = data['Sentence'].tolist()\n",
    "labels = data['Tag'].tolist()\n",
    "tokenized_data = tokenize_and_align_labels(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the dataset into train and test\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict(tokenized_data)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 28:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.112916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.090599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.085683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.089177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.093740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.104040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.110552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.120481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.126349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.130697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.10264405244092147, metrics={'train_runtime': 1690.4703, 'train_samples_per_second': 226.943, 'train_steps_per_second': 1.775, 'total_flos': 3.054720161911392e+16, 'train_loss': 0.10264405244092147, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model for finetune\n",
    "\n",
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "num_labels = len(set([label for sublist in data['Tag'].tolist() for label in sublist]))\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=10,\n",
    "    run_name='ner_experiment_1',\n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",   \n",
    "    save_strategy=\"epoch\",            # Save at the end of each epoch\n",
    "    save_total_limit=1    \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/arunkumar12/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13069701194763184,\n",
       " 'eval_runtime': 13.5518,\n",
       " 'eval_samples_per_second': 707.73,\n",
       " 'eval_steps_per_second': 1.402,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/tokenizer_config.json',\n",
       " './results/special_tokens_map.json',\n",
       " './results/vocab.txt',\n",
       " './results/added_tokens.json',\n",
       " './results/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_path = './results'  # Path where your fine-tuned model is saved\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def ner_inference(sentence):\n",
    "    # Tokenize input sentence and prepare input tensor\n",
    "    inputs = tokenizer(sentence.split(), return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    \n",
    "    # Get predicted label IDs and convert them to tag names\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    predicted_labels = predictions[0].cpu().numpy()\n",
    "    \n",
    "    # Convert IDs to tags using id_to_tag mapping\n",
    "    tokens = inputs['input_ids'][0].cpu().numpy()\n",
    "    tags = [id_to_tag[id] for id in predicted_labels]\n",
    "    \n",
    "    # Combine tokens with their predicted tags\n",
    "    results = []\n",
    "    for token_id, tag in zip(tokens, tags):\n",
    "        token = tokenizer.decode([token_id]).strip()\n",
    "        if token not in [\"[CLS]\", \"[SEP]\"]:  # Skip special tokens\n",
    "            results.append((token, tag))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Results:\n",
      "Nothing: O\n",
      "is: O\n",
      "there: O\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Nothing is there\"\n",
    "results = ner_inference(sentence)\n",
    "\n",
    "print(\"NER Results:\")\n",
    "for token, tag in results:\n",
    "    print(f\"{token}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
